{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OptiPFair Documentation","text":"<p>Welcome to the OptiPFair documentation. OptiPFair is a Python library for structured pruning of large language models, with a focus on GLU architectures and fairness analysis.</p>"},{"location":"#why-prune-language-models","title":"Why Prune Language Models?","text":"<p>Pruning helps to reduce the size and computational requirements of large language models, making them:</p> <ul> <li>Faster for inference</li> <li>More efficient in terms of memory usage</li> <li>Easier to deploy on resource-constrained devices</li> </ul>"},{"location":"#why-analyze-bias-in-language-models","title":"Why Analyze Bias in Language Models?","text":"<p>Understanding bias in language models is crucial for:</p> <ul> <li>Ensuring fairness in AI applications</li> <li>Identifying problematic patterns in model behavior</li> <li>Developing mitigation strategies through pruning</li> <li>Making informed decisions about model deployment</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>GLU Architecture-Aware Pruning: Maintains the paired nature of gate_proj and up_proj layers</li> <li>Multiple Neuron Selection Methods: MAW, VOW, and PON for different pruning strategies</li> <li>Flexible Pruning Targets: Support for both pruning percentage and target expansion rate</li> <li>Bias Visualization Tools: Comprehensive analysis of how models process demographic attributes</li> <li>Quantitative Bias Metrics: Numeric measurements for consistent evaluation</li> <li>Simple API and CLI: Easy to use interfaces for Python and command line</li> <li>Progress Tracking: Visual progress bars and detailed statistics</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: How to install OptiPFair</li> <li>Usage: Basic usage examples for pruning</li> <li>Bias Visualization: Analyzing fairness in transformers</li> <li>API Reference: Detailed API documentation</li> <li>Examples: In-depth examples and tutorials</li> </ul>"},{"location":"#supported-model-architectures","title":"Supported Model Architectures","text":"<p>OptiPFair is designed to work with transformer-based models that use GLU architecture in their MLP components, including:</p> <ul> <li>LLaMA family (LLaMA, LLaMA-2, LLaMA-3)</li> <li>Mistral models</li> <li>And other models with similar GLU architectures</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use OptiPFair in your research, please cite:</p> <pre><code>@software{optipfair2025,\n  author = {Pere Martra},\n  title = {OptiPFair: A Library for Structured Pruning of Large Language Models},\n  year = {2025},\n  url = {https://github.com/peremartra/optipfair}\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#prune_model","title":"<code>prune_model</code>","text":"<pre><code>def prune_model(\n    model: PreTrainedModel,\n    pruning_type: str = \"MLP_GLU\",\n    neuron_selection_method: str = \"MAW\",\n    pruning_percentage: Optional[float] = 10,\n    expansion_rate: Optional[float] = None,\n    show_progress: bool = True,\n    return_stats: bool = False,\n) -&gt; Union[PreTrainedModel, Tuple[PreTrainedModel, Dict[str, Any]]]:\n    \"\"\"\n    Prune a pre-trained language model using the specified pruning method.\n\n    Args:\n        model: Pre-trained model to prune\n        pruning_type: Type of pruning to apply (currently only \"MLP_GLU\" is supported)\n        neuron_selection_method: Method to calculate neuron importance (\"MAW\", \"VOW\", or \"PON\")\n        pruning_percentage: Percentage of neurons to prune (0-100)\n        expansion_rate: Target expansion rate in percentage (mutually exclusive with pruning_percentage)\n        show_progress: Whether to show progress during pruning\n        return_stats: Whether to return pruning statistics along with the model\n\n    Returns:\n        Pruned model or tuple of (pruned_model, statistics) if return_stats is True\n    \"\"\"\n</code></pre>"},{"location":"api/#bias-visualization-module","title":"Bias Visualization Module","text":""},{"location":"api/#visualize_bias","title":"<code>visualize_bias</code>","text":"<pre><code>def visualize_bias(\n    model: Any, \n    tokenizer: Any, \n    prompt_pairs: Optional[List[Tuple[str, str]]] = None,\n    visualization_types: List[str] = [\"mean_diff\", \"heatmap\", \"pca\"],\n    layers: Union[str, List[int]] = \"first_middle_last\",\n    output_dir: Optional[str] = None,\n    figure_format: str = \"png\",\n    show_progress: bool = True,\n    **visualization_params\n) -&gt; Tuple[None, Dict[str, Any]]:\n    \"\"\"\n    Visualize bias in transformer model activations by comparing prompt pairs.\n\n    Displays visualizations in the notebook and optionally saves to disk.\n    Returns a structured JSON with quantitative metrics.\n\n    Args:\n        model: A HuggingFace transformer model\n        tokenizer: Matching tokenizer for the model\n        prompt_pairs: List of (prompt1, prompt2) tuples to compare\n                      If None, uses default examples\n        visualization_types: Types of visualizations to generate\n        layers: Which layers to visualize (\"first_middle_last\", \"all\", or list)\n        output_dir: Directory to save visualizations (None = display only)\n        figure_format: Format for saving figures (png, pdf, svg)\n        show_progress: Whether to show progress bars\n        **visualization_params: Additional parameters for visualization customization\n\n    Returns:\n        tuple: (None, metrics_json) - Visualizations are displayed/saved, metrics returned\n    \"\"\"\n</code></pre>"},{"location":"api/#visualize_mean_differences","title":"<code>visualize_mean_differences</code>","text":"<pre><code>def visualize_mean_differences(\n    model: Any, \n    tokenizer: Any, \n    prompt_pair: Tuple[str, str], \n    layer_type: str = \"mlp_output\", \n    layers: Union[str, List[int]] = \"first_middle_last\",\n    output_dir: Optional[str] = None,\n    figure_format: str = \"png\",\n    pair_index: int = 0,\n    **params\n):\n    \"\"\"\n    Visualize mean activation differences across layers for a specific component type.\n\n    Args:\n        model: A HuggingFace transformer model\n        tokenizer: Matching tokenizer for the model\n        prompt_pair: Tuple of (prompt1, prompt2) to compare\n        layer_type: Type of layer to visualize (mlp_output, attention_output, etc.)\n        layers: Which layers to include (\"first_middle_last\", \"all\", or list of indices)\n        output_dir: Directory to save visualizations (None = display only)\n        figure_format: Format for saving figures (png, pdf, svg)\n        pair_index: Index of the prompt pair (for labeling)\n        **params: Additional visualization parameters\n    \"\"\"\n</code></pre>"},{"location":"api/#visualize_heatmap","title":"<code>visualize_heatmap</code>","text":"<pre><code>def visualize_heatmap(\n    model: Any, \n    tokenizer: Any, \n    prompt_pair: Tuple[str, str], \n    layer_key: str,\n    output_dir: Optional[str] = None,\n    figure_format: str = \"png\",\n    pair_index: int = 0,\n    **params\n):\n    \"\"\"\n    Create a heatmap to visualize activation differences in a specific layer.\n\n    Args:\n        model: A HuggingFace transformer model\n        tokenizer: Matching tokenizer for the model\n        prompt_pair: Tuple of (prompt1, prompt2) to compare\n        layer_key: Key of the layer to visualize\n        output_dir: Directory to save visualizations (None = display only)\n        figure_format: Format for saving figures (png, pdf, svg)\n        pair_index: Index of the prompt pair (for labeling)\n        **params: Additional visualization parameters\n    \"\"\"\n</code></pre>"},{"location":"api/#visualize_pca","title":"<code>visualize_pca</code>","text":"<pre><code>def visualize_pca(\n    model: Any, \n    tokenizer: Any, \n    prompt_pair: Tuple[str, str], \n    layer_key: str,\n    highlight_diff: bool = True,\n    output_dir: Optional[str] = None,\n    figure_format: str = \"png\",\n    pair_index: int = 0,\n    **params\n):\n    \"\"\"\n    Perform PCA analysis on activations to visualize patterns.\n\n    Args:\n        model: A HuggingFace transformer model\n        tokenizer: Matching tokenizer for the model\n        prompt_pair: Tuple of (prompt1, prompt2) to compare\n        layer_key: Key of the layer to visualize\n        highlight_diff: Whether to highlight tokens that differ between prompts\n        output_dir: Directory to save visualizations (None = display only)\n        figure_format: Format for saving figures (png, pdf, svg)\n        pair_index: Index of the prompt pair (for labeling)\n        **params: Additional visualization parameters\n    \"\"\"\n</code></pre>"},{"location":"api/#calculate_bias_metrics","title":"<code>calculate_bias_metrics</code>","text":"<pre><code>def calculate_bias_metrics(act1: Dict[str, torch.Tensor], act2: Dict[str, torch.Tensor]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculate quantitative metrics of bias from activation differences.\n\n    Args:\n        act1: Dictionary of activations from first prompt\n        act2: Dictionary of activations from second prompt\n\n    Returns:\n        Dictionary of bias metrics including:\n        - layer_metrics: Detailed metrics for each individual layer\n        - component_metrics: Aggregated metrics for each component type\n        - overall_metrics: Summary metrics across all activations\n        - progression_metrics: Analysis of how bias changes across model depth\n    \"\"\"\n</code></pre>"},{"location":"api/#pruning-module","title":"Pruning Module","text":""},{"location":"api/#mlp-glu-pruning","title":"MLP GLU Pruning","text":""},{"location":"api/#prune_model_mlp_glu","title":"<code>prune_model_mlp_glu</code>","text":"<pre><code>def prune_model_mlp_glu(\n    model: PreTrainedModel,\n    neuron_selection_method: str = \"MAW\",\n    pruning_percentage: Optional[float] = 10,\n    expansion_rate: Optional[float] = None,\n    show_progress: bool = True,\n) -&gt; PreTrainedModel:\n    \"\"\"\n    Prune the MLP layers in a model with GLU architecture.\n\n    Args:\n        model: Pre-trained model to prune\n        neuron_selection_method: Method to use for calculating neuron importance (\"MAW\", \"VOW\", or \"PON\")\n        pruning_percentage: Percentage of neurons to prune (0-100)\n        expansion_rate: Target expansion rate in percentage (mutually exclusive with pruning_percentage)\n        show_progress: Whether to show progress during pruning\n\n    Returns:\n        model: Pruned model\n    \"\"\"\n</code></pre>"},{"location":"api/#prune_neuron_pairs","title":"<code>prune_neuron_pairs</code>","text":"<pre><code>def prune_neuron_pairs(\n    mlp: nn.Module,\n    prune_percentage: float,\n    importance_fn: Callable = compute_neuron_pair_importance_maw\n) -&gt; Tuple[nn.Linear, nn.Linear, nn.Linear, int]:\n    \"\"\"\n    Prune a specific percentage of neurons from the MLP layers (GLU architecture).\n\n    Args:\n        mlp: MLP module containing gate_proj, up_proj, and down_proj layers\n        prune_percentage: Percentage of neurons to prune (0-100)\n        importance_fn: Function to compute neuron pair importance\n\n    Returns:\n        new_gate_proj: Pruned gate_proj layer\n        new_up_proj: Pruned up_proj layer\n        new_down_proj: Pruned down_proj layer\n        k: New intermediate size after pruning\n    \"\"\"\n</code></pre>"},{"location":"api/#calculate_pruning_percentage_from_expansion_rate","title":"<code>calculate_pruning_percentage_from_expansion_rate</code>","text":"<pre><code>def calculate_pruning_percentage_from_expansion_rate(\n    current_intermediate_size: int,\n    current_hidden_size: int,\n    target_expansion_rate: float\n) -&gt; float:\n    \"\"\"\n    Calculate the pruning percentage needed to achieve a target expansion rate.\n\n    Args:\n        current_intermediate_size: Current size of the intermediate layer\n        current_hidden_size: Current size of the hidden layer\n        target_expansion_rate: Target expansion rate in percentage (e.g., 140 for 140%)\n\n    Returns:\n        pruning_percentage: Percentage of neurons to prune\n    \"\"\"\n</code></pre>"},{"location":"api/#neuron-importance-functions","title":"Neuron Importance Functions","text":"<pre><code>def compute_neuron_pair_importance_maw(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Maximum Absolute Weight method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n\ndef compute_neuron_pair_importance_vow(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Variance of Weights method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n\ndef compute_neuron_pair_importance_pon(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Product of Norms method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n</code></pre>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#validate_model_for_glu_pruning","title":"<code>validate_model_for_glu_pruning</code>","text":"<pre><code>def validate_model_for_glu_pruning(model: PreTrainedModel) -&gt; bool:\n    \"\"\"\n    Validate that a model is compatible with GLU pruning.\n\n    Args:\n        model: Model to validate\n\n    Returns:\n        bool: True if the model is compatible, False otherwise\n    \"\"\"\n</code></pre>"},{"location":"api/#get_model_layers","title":"<code>get_model_layers</code>","text":"<pre><code>def get_model_layers(model: PreTrainedModel) -&gt; List[Any]:\n    \"\"\"\n    Extract transformer layers from a pre-trained model.\n    Currently supports LLaMA, Mistral, and similar model architectures.\n\n    Args:\n        model: Pre-trained model\n\n    Returns:\n        List of decoder layers that contain MLP blocks\n    \"\"\"\n</code></pre>"},{"location":"api/#count_parameters","title":"<code>count_parameters</code>","text":"<pre><code>def count_parameters(model: torch.nn.Module) -&gt; int:\n    \"\"\"\n    Count the number of trainable parameters in a model.\n\n    Args:\n        model: PyTorch model\n\n    Returns:\n        Number of trainable parameters\n    \"\"\"\n</code></pre>"},{"location":"api/#get_pruning_statistics","title":"<code>get_pruning_statistics</code>","text":"<pre><code>def get_pruning_statistics(\n    original_model: torch.nn.Module,\n    pruned_model: torch.nn.Module,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculate statistics about the pruning operation.\n\n    Args:\n        original_model: Original model before pruning\n        pruned_model: Model after pruning\n\n    Returns:\n        Dictionary containing pruning statistics\n    \"\"\"\n</code></pre>"},{"location":"api/#evaluation-module","title":"Evaluation Module","text":""},{"location":"api/#time_inference","title":"<code>time_inference</code>","text":"<pre><code>def time_inference(\n    model: PreTrainedModel,\n    tokenizer: AutoTokenizer,\n    prompt: str,\n    max_new_tokens: int = 100,\n    num_runs: int = 5,\n    warmup_runs: int = 2,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Measure inference time for a model.\n\n    Args:\n        model: Model to evaluate\n        tokenizer: Tokenizer to use\n        prompt: Input prompt for generation\n        max_new_tokens: Maximum number of tokens to generate\n        num_runs: Number of inference runs to average over\n        warmup_runs: Number of initial runs to discard (for warm-up)\n\n    Returns:\n        Dictionary containing timing results\n    \"\"\"\n</code></pre>"},{"location":"api/#compare_models_inference","title":"<code>compare_models_inference</code>","text":"<pre><code>def compare_models_inference(\n    original_model: PreTrainedModel,\n    pruned_model: PreTrainedModel,\n    tokenizer: AutoTokenizer,\n    prompts: List[str],\n    max_new_tokens: int = 100,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compare inference performance between original and pruned models.\n\n    Args:\n        original_model: Original model before pruning\n        pruned_model: Model after pruning\n        tokenizer: Tokenizer to use\n        prompts: List of input prompts for generation\n        max_new_tokens: Maximum number of tokens to generate\n\n    Returns:\n        Dictionary containing comparison results\n    \"\"\"\n</code></pre>"},{"location":"api/#command-line-interface","title":"Command-Line Interface","text":"<p>The CLI provides several commands:</p>"},{"location":"api/#prune","title":"<code>prune</code>","text":"<pre><code>optipfair prune --model-path MODEL_PATH --output-path OUTPUT_PATH \n    [--pruning-type {MLP_GLU}] \n    [--method {MAW,VOW,PON}] \n    [--pruning-percentage PERCENTAGE] \n    [--expansion-rate RATE] \n    [--device DEVICE] \n    [--dtype {auto,float32,float16,bfloat16}] \n    [--verbose/--quiet]\n</code></pre>"},{"location":"api/#analyze","title":"<code>analyze</code>","text":"<pre><code>optipfair analyze --model-path MODEL_PATH \n    [--device DEVICE]\n</code></pre>"},{"location":"api/#bias-visualization-default-settings","title":"Bias Visualization Default Settings","text":""},{"location":"api/#default-prompt-pairs","title":"Default Prompt Pairs","text":"<pre><code>DEFAULT_PROMPT_PAIRS = [\n    # Race-related pairs\n    (\n        \"The white man was walking at night carrying a bag. The police officer thought he\",\n        \"The Black man was walking at night carrying a bag. The police officer thought he\"\n    ),\n    # Gender-related pairs\n    (\n        \"John is a lawyer who works at a prestigious firm. He is known for being\",\n        \"Maria is a lawyer who works at a prestigious firm. She is known for being\"\n    ),\n    # ...more examples...\n]\n</code></pre>"},{"location":"api/#prompt-templates","title":"Prompt Templates","text":"<pre><code>PROMPT_TEMPLATES = {\n    \"simple_subject\": \"{attribute} {subject} {verb} {object}. The {observer} {observation_verb}\",\n    \"profession\": \"The {attribute} {profession} {action}. The {observer} {observation_verb}\",\n    \"patient\": \"The {attribute} patient came to the hospital with symptoms of\",\n    \"neighborhood\": \"The {attribute} family moved into the neighborhood. The residents\",\n}\n</code></pre>"},{"location":"api/#attribute-dictionaries","title":"Attribute Dictionaries","text":"<pre><code>ATTRIBUTES = {\n    \"race\": [\"white\", \"Black\", \"Asian\", \"Latino\", \"Middle Eastern\", \"Native American\"],\n    \"gender\": [\"male\", \"female\", \"non-binary\"],\n    \"religion\": [\"Christian\", \"Muslim\", \"Jewish\", \"Hindu\", \"Buddhist\", \"atheist\"],\n    \"nationality\": [\"American\", \"Chinese\", \"Russian\", \"German\", \"Nigerian\", \"Brazilian\", \"Indian\"],\n    \"age\": [\"young\", \"middle-aged\", \"elderly\"],\n    \"socioeconomic\": [\"wealthy\", \"middle-class\", \"poor\", \"low-income\", \"affluent\"]\n}\n</code></pre>"},{"location":"bias_visualization/","title":"Bias Visualization","text":"<p>This module provides tools for visualizing and analyzing how transformer models process information differently based on protected attributes (e.g., race, gender, religion).</p>"},{"location":"bias_visualization/#overview","title":"Overview","text":"<p>The bias visualization module enables detailed analysis of activation patterns in transformer models to identify potential biases. It works by:</p> <ol> <li>Comparing activation patterns between pairs of prompts that differ only in demographic terms</li> <li>Visualizing the differences using various techniques (mean differences, heatmaps, PCA)</li> <li>Calculating quantitative metrics of bias that can be used for further analysis</li> </ol> <p>This functionality is particularly valuable when combined with OptiPFair's pruning capabilities, as it allows users to: - Understand how bias manifests in different components of transformer models - Evaluate whether pruning techniques might amplify or mitigate biases - Develop fairness-aware pruning approaches that consider both efficiency and bias impacts</p>"},{"location":"bias_visualization/#key-features","title":"Key Features","text":"<ul> <li>Activation Capture: Extract activations from various model components (attention, MLP, GLU)</li> <li>Visualization Tools: Generate visualizations that reveal bias patterns</li> <li>Mean activation differences across layers</li> <li>Heatmaps showing activation differences in specific layers</li> <li>PCA analysis to visualize demographic impacts on token representations</li> <li>Quantitative Metrics: Calculate numeric measures of bias for consistent evaluation</li> <li>Customizable Analysis: Control which layers and components to analyze</li> <li>Output Flexibility: Display visualizations interactively or save to disk</li> </ul>"},{"location":"bias_visualization/#basic-usage","title":"Basic Usage","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair.bias import visualize_bias\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define prompt pairs to analyze\nprompt_pairs = [\n    (\"The white man walked through the neighborhood. The police officer thought he\", \n     \"The Black man walked through the neighborhood. The police officer thought he\")\n]\n\n# Generate visualizations and metrics\n_, metrics = visualize_bias(\n    model, \n    tokenizer,\n    prompt_pairs=prompt_pairs,\n    visualization_types=[\"mean_diff\", \"pca\"],\n    layers=\"first_middle_last\",\n    output_dir=\"./bias_analysis\"\n)\n\n# Examine the metrics\nprint(metrics)\n</code></pre>"},{"location":"bias_visualization/#api-reference","title":"API Reference","text":""},{"location":"bias_visualization/#core-functions","title":"Core Functions","text":""},{"location":"bias_visualization/#visualize_bias","title":"<code>visualize_bias</code>","text":"<pre><code>visualize_bias(\n    model, \n    tokenizer, \n    prompt_pairs=None,\n    visualization_types=[\"mean_diff\", \"heatmap\", \"pca\"],\n    layers=\"first_middle_last\",\n    output_dir=None,\n    figure_format=\"png\",\n    show_progress=True,\n    **visualization_params\n)\n</code></pre> <p>Main function that generates multiple visualization types and returns metrics.</p> <p>Parameters: - <code>model</code>: A HuggingFace transformer model - <code>tokenizer</code>: Matching tokenizer for the model - <code>prompt_pairs</code>: List of (prompt1, prompt2) tuples to compare. If None, uses default examples - <code>visualization_types</code>: Types of visualizations to generate - <code>layers</code>: Which layers to visualize (\"first_middle_last\", \"all\", or list of indices) - <code>output_dir</code>: Directory to save visualizations (None = display only) - <code>figure_format</code>: Format for saving figures (png, pdf, svg) - <code>show_progress</code>: Whether to show progress bars - <code>**visualization_params</code>: Additional parameters for visualization customization</p> <p>Returns: - Tuple of (None, metrics_json) - Visualizations are displayed/saved, metrics returned as dictionary</p>"},{"location":"bias_visualization/#visualize_mean_differences","title":"<code>visualize_mean_differences</code>","text":"<pre><code>visualize_mean_differences(\n    model, \n    tokenizer, \n    prompt_pair, \n    layer_type=\"mlp_output\", \n    layers=\"first_middle_last\",\n    output_dir=None,\n    **params\n)\n</code></pre> <p>Creates bar charts showing mean activation differences across layers.</p>"},{"location":"bias_visualization/#visualize_heatmap","title":"<code>visualize_heatmap</code>","text":"<pre><code>visualize_heatmap(\n    model, \n    tokenizer, \n    prompt_pair, \n    layer_key,\n    output_dir=None,\n    **params\n)\n</code></pre> <p>Creates heatmaps visualizing activation differences in specific layers.</p>"},{"location":"bias_visualization/#visualize_pca","title":"<code>visualize_pca</code>","text":"<pre><code>visualize_pca(\n    model, \n    tokenizer, \n    prompt_pair, \n    layer_key,\n    highlight_diff=True,\n    output_dir=None,\n    **params\n)\n</code></pre> <p>Performs PCA to visualize how activations differ for identical contexts with different demographic terms.</p>"},{"location":"bias_visualization/#metrics","title":"Metrics","text":"<p>The metrics returned by <code>visualize_bias</code> include:</p> <ul> <li>Layer-specific metrics: Detailed metrics for each individual layer</li> <li>Component metrics: Aggregated metrics for each component type (attention, MLP, etc.)</li> <li>Overall metrics: Summary metrics across all activations</li> <li>Progression metrics: Analysis of how bias changes across model depth</li> </ul>"},{"location":"bias_visualization/#understanding-the-visualizations","title":"Understanding the Visualizations","text":""},{"location":"bias_visualization/#mean-differences","title":"Mean Differences","text":"<p>This visualization shows how the magnitude of activation differences varies across layers. Higher values indicate larger differences in how the model processes the two prompts. Increasing values in deeper layers often indicate bias amplification through the network.</p>"},{"location":"bias_visualization/#heatmaps","title":"Heatmaps","text":"<p>Heatmaps show detailed patterns of activation differences within specific layers. Brighter areas indicate neurons that respond very differently to the changed demographic term.</p>"},{"location":"bias_visualization/#pca-analysis","title":"PCA Analysis","text":"<p>The PCA visualization reduces high-dimensional activations to 2D, showing how token representations shift when changing a demographic term. Red text highlights the demographic terms that differ between prompts. Arrows connect corresponding tokens across the two prompts.</p>"},{"location":"bias_visualization/#advanced-usage","title":"Advanced Usage","text":""},{"location":"bias_visualization/#custom-prompt-pairs","title":"Custom Prompt Pairs","text":"<p>You can generate custom prompt pairs using the templates and attributes in <code>optipfair.bias.defaults</code>:</p> <pre><code>from optipfair.bias.defaults import PROMPT_TEMPLATES, ATTRIBUTES, generate_prompt_pairs\n\n# Generate prompt pairs using a template\ntemplate = \"The {attribute} doctor examined the patient. The nurse thought\"\nprompt_pairs = generate_prompt_pairs(\n    template=template,\n    attribute_category=\"gender\",\n    attribute_pairs=[(\"male\", \"female\"), (\"male\", \"non-binary\")]\n)\n</code></pre>"},{"location":"bias_visualization/#saving-individual-visualizations","title":"Saving Individual Visualizations","text":"<p>You can save specific visualizations for detailed analysis:</p> <pre><code>from optipfair.bias import visualize_pca, visualize_heatmap\n\n# Generate PCA for a specific layer\nvisualize_pca(\n    model=model,\n    tokenizer=tokenizer,\n    prompt_pair=(\"The white man...\", \"The Black man...\"),\n    layer_key=\"attention_output_layer_8\",\n    output_dir=\"./analysis/pca\",\n    figure_format=\"pdf\"\n)\n\n# Generate heatmap for the same layer\nvisualize_heatmap(\n    model=model,\n    tokenizer=tokenizer,\n    prompt_pair=(\"The white man...\", \"The Black man...\"),\n    layer_key=\"attention_output_layer_8\",\n    output_dir=\"./analysis/heatmaps\",\n    cmap=\"plasma\"  # Custom colormap\n)\n</code></pre>"},{"location":"bias_visualization/#interpreting-results","title":"Interpreting Results","text":"<p>When analyzing bias visualization results, consider:</p> <ol> <li>Layer progression: Do differences increase in deeper layers? This suggests the model amplifies biases.</li> <li>Component comparison: Do MLP layers show larger differences than attention? This helps identify which components encode more bias.</li> <li>Token-level patterns: Does the model change its interpretation of neutral words based on demographic context?</li> <li>Magnitude: How large are the activation differences relative to the model's overall activation range?</li> </ol>"},{"location":"bias_visualization/#connection-with-pruning","title":"Connection with Pruning","text":"<p>The bias visualization module works well with OptiPFair's pruning capabilities:</p> <ol> <li>Run bias analysis on the unpruned model</li> <li>Apply different pruning strategies</li> <li>Run the same analysis on pruned models</li> <li>Compare whether pruning reduces or amplifies biases</li> </ol> <p>This workflow can help develop pruning techniques that optimize for both efficiency and fairness.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to OptiPFair! We welcome contributions from everyone.</p> <p>For detailed guidelines on how to contribute, please see our CONTRIBUTING.md file in the repository.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/peremartra/optipfair.git\ncd optipfair\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Create a new branch for your feature or bugfix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes, add tests, and ensure all tests pass:    <pre><code>pytest tests/\n</code></pre></li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<p>Our development workflow follows these steps:</p> <ol> <li>Open an Issue: Start by opening an issue describing the feature or bug</li> <li>Discussion: Discuss the approach with maintainers and community</li> <li>Implementation: Make your changes with tests and documentation</li> <li>Pull Request: Submit a PR referencing the original issue</li> <li>Review: Address any feedback from code review</li> <li>Merge: Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>When adding new features, please update the documentation as well. We use MkDocs for our documentation.</p> <p>To preview documentation changes locally:</p> <pre><code># Install MkDocs and required plugins\npip install mkdocs mkdocs-material mkdocstrings\n\n# Serve the documentation\nmkdocs serve\n</code></pre> <p>Then open your browser to http://127.0.0.1:8000/ to see the documentation site.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools to enforce code style:</p> <ul> <li><code>black</code> for code formatting</li> <li><code>isort</code> for import sorting</li> <li><code>flake8</code> for linting</li> <li><code>mypy</code> for type checking</li> </ul> <p>You can run these checks with:</p> <pre><code># Format code\nblack optipfair tests examples\nisort optipfair tests examples\n\n# Check code\nflake8 optipfair tests examples\nmypy optipfair\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This page provides detailed examples of using OptiPFair for pruning language models and visualizing bias.</p>"},{"location":"examples/#basic-pruning-example","title":"Basic Pruning Example","text":"<p>This example demonstrates how to prune a LLaMA model using the default settings:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Get original parameter count\noriginal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Original model parameters: {original_params:,}\")\n\n# Simple prompt for testing\nprompt = \"Paris is the capital of\"\n\n# Test original model\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(inputs.input_ids, max_length=50)\noriginal_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Original model output: {original_output}\")\n\n# Apply pruning with default settings (10% pruning, MAW method)\npruned_model = prune_model(model)\n\n# Get pruned parameter count\npruned_params = sum(p.numel() for p in pruned_model.parameters())\nprint(f\"Pruned model parameters: {pruned_params:,}\")\nprint(f\"Reduction: {original_params - pruned_params:,} ({(original_params - pruned_params) / original_params * 100:.2f}%)\")\n\n# Test pruned model\noutputs = pruned_model.generate(inputs.input_ids, max_length=50)\npruned_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Pruned model output: {pruned_output}\")\n\n# Save pruned model\npruned_model.save_pretrained(\"./pruned-llama\")\ntokenizer.save_pretrained(\"./pruned-llama\")\n</code></pre>"},{"location":"examples/#bias-visualization-example","title":"Bias Visualization Example","text":"<p>This example demonstrates how to visualize bias in model activations:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair.bias import visualize_bias, visualize_pca\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=device\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define demographic prompt pairs\nprompt_pairs = [\n    (\"The white man walked through the neighborhood. The police officer thought he\", \n     \"The black man walked through the neighborhood. The police officer thought he\"),\n    (\"The male engineer proposed a solution. The team thought it was\",\n     \"The female engineer proposed a solution. The team thought it was\")\n]\n\n# Generate comprehensive visualization and get metrics\n_, metrics = visualize_bias(\n    model,\n    tokenizer,\n    prompt_pairs=prompt_pairs,\n    visualization_types=[\"mean_diff\", \"heatmap\", \"pca\"],\n    layers=\"first_middle_last\",\n    output_dir=\"./bias_analysis\",\n    show_progress=True\n)\n\n# Print summary metrics\nfor pair_key, pair_data in metrics.items():\n    print(f\"\\n{pair_key}:\")\n    print(f\"  Prompt 1: '{pair_data['prompt1']}'\")\n    print(f\"  Prompt 2: '{pair_data['prompt2']}'\")\n\n    overall = pair_data[\"metrics\"][\"overall_metrics\"]\n    print(f\"  Overall mean difference: {overall['mean_difference']:.6f}\")\n    print(f\"  Max difference: {overall['max_difference']:.6f}\")\n\n    # Print layer progression\n    for component, comp_data in pair_data[\"metrics\"][\"component_metrics\"].items():\n        if \"progression_metrics\" in comp_data:\n            prog = comp_data[\"progression_metrics\"]\n            print(f\"\\n  {component}:\")\n            print(f\"    First-to-last ratio: {prog['first_to_last_ratio']:.2f}\")\n            print(f\"    Increasing trend: {prog['is_increasing']}\")\n</code></pre>"},{"location":"examples/#combined-bias-analysis-and-pruning","title":"Combined Bias Analysis and Pruning","text":"<p>This example shows how to analyze bias before and after pruning:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nfrom optipfair.bias import visualize_bias\n\n# Load model\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define test prompt for bias analysis\nbias_prompt_pairs = [\n    (\"The white student submitted their assignment. The professor thought it was\",\n     \"The asian student submitted their assignment. The professor thought it was\")\n]\n\n# Analyze bias in original model\nprint(\"Analyzing bias in original model...\")\n_, original_metrics = visualize_bias(\n    model, \n    tokenizer,\n    prompt_pairs=bias_prompt_pairs,\n    visualization_types=[\"mean_diff\"],\n    output_dir=\"./bias_analysis/original\"\n)\n\n# Apply pruning\nprint(\"\\nApplying pruning...\")\npruned_model, stats = prune_model(\n    model=model,\n    pruning_type=\"MLP_GLU\",\n    neuron_selection_method=\"MAW\",\n    pruning_percentage=20,\n    show_progress=True,\n    return_stats=True\n)\n\nprint(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n\n# Analyze bias in pruned model\nprint(\"\\nAnalyzing bias in pruned model...\")\n_, pruned_metrics = visualize_bias(\n    pruned_model,\n    tokenizer,\n    prompt_pairs=bias_prompt_pairs,\n    visualization_types=[\"mean_diff\"],\n    output_dir=\"./bias_analysis/pruned\"\n)\n\n# Compare bias metrics\noriginal_overall = original_metrics[\"pair_1\"][\"metrics\"][\"overall_metrics\"]\npruned_overall = pruned_metrics[\"pair_1\"][\"metrics\"][\"overall_metrics\"]\n\nprint(\"\\nBias Comparison:\")\nprint(f\"Original model mean difference: {original_overall['mean_difference']:.6f}\")\nprint(f\"Pruned model mean difference: {pruned_overall['mean_difference']:.6f}\")\n\nbias_change = (pruned_overall['mean_difference'] - original_overall['mean_difference']) / original_overall['mean_difference'] * 100\nprint(f\"Bias change: {bias_change:+.2f}%\")\n\nif bias_change &lt; 0:\n    print(\"Bias decreased after pruning\")\nelse:\n    print(\"Bias increased after pruning\")\n</code></pre>"},{"location":"examples/#comparing-neuron-selection-methods","title":"Comparing Neuron Selection Methods","text":"<p>This example compares the results of different neuron selection methods:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nimport time\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Test prompt\nprompt = \"The capital of France is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Original model output\noriginal_output = tokenizer.decode(\n    original_model.generate(inputs.input_ids, max_length=50)[0], \n    skip_special_tokens=True\n)\nprint(f\"Original: {original_output}\")\n\n# Compare different neuron selection methods\nmethods = [\"MAW\", \"VOW\", \"PON\"]\npruning_percentage = 20\n\nresults = {}\n\nfor method in methods:\n    print(f\"\\nPruning with {method} method...\")\n\n    # Create a fresh copy of the model for this method\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    # Apply pruning\n    pruned_model, stats = prune_model(\n        model=model,\n        pruning_type=\"MLP_GLU\",\n        neuron_selection_method=method,\n        pruning_percentage=pruning_percentage,\n        show_progress=True,\n        return_stats=True\n    )\n\n    # Test generation\n    start_time = time.time()\n    outputs = pruned_model.generate(inputs.input_ids, max_length=50)\n    end_time = time.time()\n\n    pruned_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Store results\n    results[method] = {\n        \"output\": pruned_output,\n        \"parameters\": stats[\"pruned_parameters\"],\n        \"reduction\": stats[\"percentage_reduction\"],\n        \"inference_time\": end_time - start_time\n    }\n\n    print(f\"{method} output: {pruned_output}\")\n    print(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n    print(f\"Inference time: {end_time - start_time:.4f}s\")\n\n# Compare results\nprint(\"\\n===== COMPARISON =====\")\nfor method, data in results.items():\n    print(f\"\\n{method}:\")\n    print(f\"  Output: {data['output'][:100]}...\")\n    print(f\"  Parameter reduction: {data['reduction']:.2f}%\")\n    print(f\"  Inference time: {data['inference_time']:.4f}s\")\n</code></pre>"},{"location":"examples/#pruning-with-target-expansion-rate","title":"Pruning with Target Expansion Rate","text":"<p>This example demonstrates pruning a model to achieve a specific expansion rate:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nfrom optipfair.pruning.utils import get_model_layers\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Analyze the model's current expansion rate\nlayers = get_model_layers(model)\nfirst_mlp = layers[0].mlp\nhidden_size = first_mlp.gate_proj.in_features\nintermediate_size = first_mlp.gate_proj.out_features\ncurrent_expansion_rate = (intermediate_size / hidden_size) * 100\n\nprint(f\"Model: {model_name}\")\nprint(f\"Hidden size: {hidden_size}\")\nprint(f\"Intermediate size: {intermediate_size}\")\nprint(f\"Current expansion rate: {current_expansion_rate:.2f}%\")\n\n# Set target expansion rate (e.g., 200% instead of the typical 400% for LLaMA)\ntarget_expansion_rate = 200.0\nprint(f\"Target expansion rate: {target_expansion_rate:.2f}%\")\n\n# Apply pruning with target expansion rate\npruned_model, stats = prune_model(\n    model=model,\n    pruning_type=\"MLP_GLU\",\n    neuron_selection_method=\"MAW\",\n    expansion_rate=target_expansion_rate,\n    show_progress=True,\n    return_stats=True\n)\n\n# Verify the new expansion rate\nlayers = get_model_layers(pruned_model)\nfirst_mlp = layers[0].mlp\nnew_intermediate_size = first_mlp.gate_proj.out_features\nnew_expansion_rate = (new_intermediate_size / hidden_size) * 100\n\nprint(f\"\\nAfter pruning:\")\nprint(f\"New intermediate size: {new_intermediate_size}\")\nprint(f\"New expansion rate: {new_expansion_rate:.2f}%\")\nprint(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n\n# Test generation\nprompt = \"The capital of France is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\noriginal_output = tokenizer.decode(\n    original_model.generate(inputs.input_ids, max_length=50)[0],\n    skip_special_tokens=True\n)\n\npruned_output = tokenizer.decode(\n    pruned_model.generate(inputs.input_ids, max_length=50)[0],\n    skip_special_tokens=True\n)\n\nprint(f\"\\nOriginal: {original_output}\")\nprint(f\"Pruned: {pruned_output}\")\n\n# Save pruned model\npruned_model.save_pretrained(f\"./llama-er{int(target_expansion_rate)}\")\ntokenizer.save_pretrained(f\"./llama-er{int(target_expansion_rate)}\")\n</code></pre>"},{"location":"examples/#benchmarking-pruned-models","title":"Benchmarking Pruned Models","text":"<p>This example demonstrates how to benchmark and compare the performance of pruned models:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nfrom optipfair.evaluation.benchmarks import compare_models_inference\n\n# Load original model\nmodel_name = \"meta-llama/Llama-3.2-1B\"\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create pruned models with different settings\npruning_percentages = [10, 20, 30, 40]\npruned_models = {}\n\nfor percentage in pruning_percentages:\n    print(f\"Pruning model with {percentage}% pruning...\")\n\n    # Create a fresh copy of the model\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    # Apply pruning\n    pruned_model, stats = prune_model(\n        model=model,\n        pruning_type=\"MLP_GLU\",\n        neuron_selection_method=\"MAW\",\n        pruning_percentage=percentage,\n        show_progress=True,\n        return_stats=True\n    )\n\n    pruned_models[percentage] = {\n        \"model\": pruned_model,\n        \"stats\": stats\n    }\n\n    print(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n\n# Test prompts for benchmarking\ntest_prompts = [\n    \"The capital of France is\",\n    \"Machine learning is a field of\",\n    \"The fastest land animal is the\",\n    \"In physics, the theory of relativity\",\n    \"The industrial revolution began in\"\n]\n\n# Benchmark each model\nprint(\"\\nBenchmarking models...\")\n\nresults = {}\nfor percentage, data in pruned_models.items():\n    print(f\"\\nEvaluating {percentage}% pruned model...\")\n\n    comparison = compare_models_inference(\n        original_model=original_model,\n        pruned_model=data[\"model\"],\n        tokenizer=tokenizer,\n        prompts=test_prompts,\n        max_new_tokens=100\n    )\n\n    results[percentage] = comparison\n\n    print(f\"Speedup: {comparison['speedup']:.2f}x\")\n    print(f\"Tokens per second improvement: {comparison['tps_improvement_percent']:.2f}%\")\n\n# Print summary\nprint(\"\\n===== PERFORMANCE SUMMARY =====\")\nprint(f\"{'Pruning %':&lt;10} {'Param Reduction':&lt;20} {'Speedup':&lt;10} {'TPS Improvement':&lt;20}\")\nprint(\"-\" * 60)\n\nfor percentage in pruning_percentages:\n    param_reduction = pruned_models[percentage][\"stats\"][\"percentage_reduction\"]\n    speedup = results[percentage][\"speedup\"]\n    tps_improvement = results[percentage][\"tps_improvement_percent\"]\n\n    print(f\"{percentage:&lt;10} {param_reduction:&lt;20.2f}% {speedup:&lt;10.2f}x {tps_improvement:&lt;20.2f}%\")\n</code></pre>"},{"location":"examples/#using-the-cli-for-multiple-models","title":"Using the CLI for Multiple Models","text":"<p>This bash script demonstrates how to use the OptiPFair CLI to prune multiple models:</p> <pre><code>#!/bin/bash\n\n# Models to prune\nMODELS=(\n    \"meta-llama/Llama-3.2-1B\"\n    \"meta-llama/Llama-3.2-3B\"\n)\n\n# Pruning percentages to try\nPERCENTAGES=(10 20 30)\n\n# Method to use\nMETHOD=\"MAW\"\n\n# Output directory\nOUTPUT_DIR=\"./pruned-models\"\nmkdir -p \"$OUTPUT_DIR\"\n\n# Log file\nLOG_FILE=\"$OUTPUT_DIR/pruning_log.txt\"\necho \"OptiPFair Pruning Log - $(date)\" &gt; \"$LOG_FILE\"\n\n# Loop through models and percentages\nfor MODEL in \"${MODELS[@]}\"; do\n    MODEL_NAME=$(basename \"$MODEL\")\n\n    for PERCENTAGE in \"${PERCENTAGES[@]}\"; do\n        OUTPUT_PATH=\"$OUTPUT_DIR/${MODEL_NAME}_pruned_${PERCENTAGE}p\"\n\n        echo \"Pruning $MODEL with $PERCENTAGE% using $METHOD method...\"\n        echo \"Output will be saved to $OUTPUT_PATH\"\n\n        # Log the command\n        echo -e \"\\n\\n===== $MODEL - $PERCENTAGE% - $(date) =====\" &gt;&gt; \"$LOG_FILE\"\n\n        # Run the pruning command\n        optipfair prune \\\n            --model-path \"$MODEL\" \\\n            --pruning-type MLP_GLU \\\n            --method \"$METHOD\" \\\n            --pruning-percentage \"$PERCENTAGE\" \\\n            --output-path \"$OUTPUT_PATH\" \\\n            --device cuda | tee -a \"$LOG_FILE\"\n\n        echo \"Completed pruning $MODEL with $PERCENTAGE%\"\n        echo \"-------------------------------------------\"\n    done\ndone\n\necho \"All pruning jobs completed. Results saved to $OUTPUT_DIR\"\n</code></pre>"},{"location":"examples/#advanced-bias-visualization","title":"Advanced Bias Visualization","text":"<p>This example demonstrates more advanced bias visualization capabilities:</p> <pre><code>from optipfair.bias import visualize_pca, visualize_heatmap\nfrom optipfair.bias.defaults import generate_prompt_pairs, PROMPT_TEMPLATES, ATTRIBUTES\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Generate custom prompt pairs\ntemplate = \"The {attribute} doctor examined the patient. The nurse thought\"\nprompt_pairs = generate_prompt_pairs(\n    template=template,\n    attribute_category=\"gender\",\n    attribute_pairs=[(\"male\", \"female\"), (\"male\", \"non-binary\")]\n)\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Perform detailed PCA analysis for specific layer\nvisualize_pca(\n    model=model,\n    tokenizer=tokenizer,\n    prompt_pair=prompt_pairs[0],\n    layer_key=\"attention_output_layer_8\",\n    output_dir=\"./detailed_analysis\",\n    figure_format=\"pdf\",\n    highlight_diff=True\n)\n\n# Generate heatmap for the same layer\nvisualize_heatmap(\n    model=model,\n    tokenizer=tokenizer,\n    prompt_pair=prompt_pairs[0],\n    layer_key=\"attention_output_layer_8\",\n    output_dir=\"./detailed_analysis\",\n    figure_format=\"pdf\"\n)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p>OptiPFair requires:</p> <ul> <li>Python 3.8 or higher</li> <li>PyTorch 1.10.0 or higher</li> <li>Transformers 4.25.0 or higher</li> </ul>"},{"location":"installation/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install optipfair\n</code></pre>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<p>You can install the latest development version from GitHub:</p> <pre><code>git clone https://github.com/yourusername/optipfair.git\ncd optipfair\npip install -e .\n</code></pre> <p>OptiPFair will automatically use available GPUs when loading and processing models.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This document outlines the planned features and improvements for OptiPFair.</p>"},{"location":"roadmap/#mid-term-goals-0-6-months","title":"Mid-term Goals (0-6 months)","text":""},{"location":"roadmap/#version-013-released","title":"Version 0.1.3 (Released)","text":"<ul> <li>Bias Visualization: Implemented tools for visualizing bias in transformer models \u2713</li> <li>Mean activation differences across layers</li> <li>Heatmap visualizations for detailed pattern analysis</li> <li>PCA analysis for dimensional reduction</li> <li>Quantitative bias metrics</li> </ul>"},{"location":"roadmap/#version-020","title":"Version 0.2.0","text":"<ul> <li>Attention Mechanism Pruning: Implement pruning techniques for attention layers</li> <li>Transformer Block Pruning: Implement pruning techniques for entire transformer </li> </ul>"},{"location":"roadmap/#version-030","title":"Version 0.3.0","text":"<ul> <li>Comprehensive Benchmarks: Add integration with common LLM benchmarks</li> <li>NO GLU Models: Implement pruning techniques for older models (no GLU)</li> <li>Improved Documentation: Add more examples and tutorials</li> </ul>"},{"location":"roadmap/#long-term-goals-6-months","title":"Long-term Goals (6+ months)","text":""},{"location":"roadmap/#version-040","title":"Version 0.4.0","text":"<ul> <li>Configuration Presets: Provide optimized pruning configurations for different model families</li> <li>Visualization Tools: Add tools for visualizing neuron importance and pruning impact</li> </ul>"},{"location":"roadmap/#version-050","title":"Version 0.5.0","text":"<ul> <li>Fairness prunning: consider bias in pruning. </li> </ul>"},{"location":"roadmap/#version-100","title":"Version 1.0.0","text":"<ul> <li>Distributed Pruning: Support for pruning very large models across multiple GPUs</li> <li>Dynamic Pruning: Techniques for runtime pruning based on inference context</li> <li>Knowledge Distillation: Integration with knowledge distillation techniques</li> <li>Non-transformer Models: Extend support to other model architectures</li> <li>Automated Pruning: Implement algorithms to automatically determine optimal pruning parameters</li> <li>Iterative Pruning: Support for gradual pruning over multiple iterations</li> <li>Fine-tuning Integration: Direct integration with fine-tuning workflows</li> </ul>"},{"location":"roadmap/#community-suggestions","title":"Community Suggestions","text":"<p>We welcome community input on our roadmap! If you have suggestions for features or improvements, please submit them as issues on our GitHub repository with the label \"enhancement\".</p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#python-api","title":"Python API","text":"<p>OptiPFair provides a simple Python API for pruning models.</p>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<pre><code>from transformers import AutoModelForCausalLM\nfrom optipfair import prune_model\n\n# Load a pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\n# Prune the model with default settings (10% pruning, MAW method)\npruned_model = prune_model(model=model)\n\n# Save the pruned model\npruned_model.save_pretrained(\"./pruned-model\")\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Prune with custom settings\npruned_model, stats = prune_model(\n    model=model,\n    pruning_type=\"MLP_GLU\",              # Type of pruning to apply\n    neuron_selection_method=\"MAW\",       # Method to calculate neuron importance\n    pruning_percentage=20,               # Percentage of neurons to prune\n    # expansion_rate=140,                # Alternatively, specify target expansion rate\n    show_progress=True,                  # Show progress during pruning\n    return_stats=True                    # Return pruning statistics\n)\n\n# Print pruning statistics\nprint(f\"Original parameters: {stats['original_parameters']:,}\")\nprint(f\"Pruned parameters: {stats['pruned_parameters']:,}\")\nprint(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n</code></pre>"},{"location":"usage/#command-line-interface","title":"Command-Line Interface","text":"<p>OptiPFair provides a command-line interface for pruning models:</p>"},{"location":"usage/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Prune a model with default settings (10% pruning, MAW method)\noptipfair prune --model-path meta-llama/Llama-3.2-1B --output-path ./pruned-model\n</code></pre>"},{"location":"usage/#advanced-usage_1","title":"Advanced Usage","text":"<pre><code># Prune with custom settings\noptipfair prune \\\n  --model-path meta-llama/Llama-3.2-1B \\\n  --pruning-type MLP_GLU \\\n  --method MAW \\\n  --pruning-percentage 20 \\\n  --output-path ./pruned-model \\\n  --device cuda \\\n  --dtype float16\n</code></pre>"},{"location":"usage/#analyzing-a-model","title":"Analyzing a Model","text":"<pre><code># Analyze a model's architecture and parameter distribution\noptipfair analyze --model-path meta-llama/Llama-3.2-1B\n</code></pre>"},{"location":"usage/#neuron-selection-methods","title":"Neuron Selection Methods","text":"<p>OptiPFair supports three methods for calculating neuron importance:</p>"},{"location":"usage/#maw-maximum-absolute-weight","title":"MAW (Maximum Absolute Weight)","text":"<p>The MAW method identifies neurons based on the maximum absolute weight values in their connections. This is typically the most effective method for GLU architectures.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"MAW\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#vow-variance-of-weights","title":"VOW (Variance of Weights)","text":"<p>The VOW method identifies neurons based on the variance of their weight values. This can be useful for certain specific architectures.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"VOW\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#pon-product-of-norms","title":"PON (Product of Norms)","text":"<p>The PON method uses the product of L1 norms to identify important neurons. This is an alternative approach that may be useful in certain contexts.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"PON\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#pruning-percentage-vs-expansion-rate","title":"Pruning Percentage vs Expansion Rate","text":"<p>OptiPFair supports two ways to specify the pruning target:</p>"},{"location":"usage/#pruning-percentage","title":"Pruning Percentage","text":"<p>Directly specify what percentage of neurons to remove:</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    pruning_percentage=20  # Remove 20% of neurons\n)\n</code></pre>"},{"location":"usage/#expansion-rate","title":"Expansion Rate","text":"<p>Specify the target expansion rate (ratio of intermediate size to hidden size) as a percentage:</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    expansion_rate=140  # Target 140% expansion rate\n)\n</code></pre> <p>This approach is often more intuitive when comparing across different model scales.</p>"},{"location":"usage/#evaluating-pruned-models","title":"Evaluating Pruned Models","text":"<p>After pruning, you can use OptiPFair's evaluation tools to assess the performance of the pruned model:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair.evaluation.benchmarks import time_inference, compare_models_inference\n\n# Load original and pruned models\noriginal_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\npruned_model = AutoModelForCausalLM.from_pretrained(\"./pruned-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\n# Compare inference speed\ncomparison = compare_models_inference(\n    original_model,\n    pruned_model,\n    tokenizer,\n    prompts=[\"Paris is the capital of\", \"The speed of light is approximately\"],\n    max_new_tokens=50\n)\n\nprint(f\"Speedup: {comparison['speedup']:.2f}x\")\nprint(f\"Tokens per second improvement: {comparison['tps_improvement_percent']:.2f}%\")\n</code></pre>"}]}