{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OptiPFair Documentation","text":"<p>Welcome to the OptiPFair documentation. OptiPFair is a Python library for structured pruning of large language models, with a focus on GLU architectures.</p>"},{"location":"#why-prune-language-models","title":"Why Prune Language Models?","text":"<p>Pruning helps to reduce the size and computational requirements of large language models, making them:</p> <ul> <li>Faster for inference</li> <li>More efficient in terms of memory usage</li> <li>Easier to deploy on resource-constrained devices</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>GLU Architecture-Aware Pruning: Maintains the paired nature of gate_proj and up_proj layers</li> <li>Multiple Neuron Selection Methods: MAW, VOW, and PON for different pruning strategies</li> <li>Flexible Pruning Targets: Support for both pruning percentage and target expansion rate</li> <li>Simple API and CLI: Easy to use interfaces for Python and command line</li> <li>Progress Tracking: Visual progress bars and detailed statistics</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation: How to install OptiPFair</li> <li>Usage: Basic usage examples</li> <li>API Reference: Detailed API documentation</li> <li>Examples: In-depth examples and tutorials</li> </ul>"},{"location":"#supported-model-architectures","title":"Supported Model Architectures","text":"<p>OptiPFair is designed to work with transformer-based models that use GLU architecture in their MLP components, including:</p> <ul> <li>LLaMA family (LLaMA, LLaMA-2, LLaMA-3)</li> <li>Mistral models</li> <li>And other models with similar GLU architectures</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use OptiPFair in your research, please cite:</p> <pre><code>@software{optipfair2025,\n  author = {Pere Martra},\n  title = {OptiPFair: A Library for Structured Pruning of Large Language Models},\n  year = {2025},\n  url = {https://github.com/yourusername/optipfair}\n}\n</code></pre>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#prune_model","title":"<code>prune_model</code>","text":"<pre><code>def prune_model(\n    model: PreTrainedModel,\n    pruning_type: str = \"MLP_GLU\",\n    neuron_selection_method: str = \"MAW\",\n    pruning_percentage: Optional[float] = 10,\n    expansion_rate: Optional[float] = None,\n    show_progress: bool = True,\n    return_stats: bool = False,\n) -&gt; Union[PreTrainedModel, Tuple[PreTrainedModel, Dict[str, Any]]]:\n    \"\"\"\n    Prune a pre-trained language model using the specified pruning method.\n\n    Args:\n        model: Pre-trained model to prune\n        pruning_type: Type of pruning to apply (currently only \"MLP_GLU\" is supported)\n        neuron_selection_method: Method to calculate neuron importance (\"MAW\", \"VOW\", or \"PON\")\n        pruning_percentage: Percentage of neurons to prune (0-100)\n        expansion_rate: Target expansion rate in percentage (mutually exclusive with pruning_percentage)\n        show_progress: Whether to show progress during pruning\n        return_stats: Whether to return pruning statistics along with the model\n\n    Returns:\n        Pruned model or tuple of (pruned_model, statistics) if return_stats is True\n    \"\"\"\n</code></pre>"},{"location":"api/#pruning-module","title":"Pruning Module","text":""},{"location":"api/#mlp-glu-pruning","title":"MLP GLU Pruning","text":""},{"location":"api/#prune_model_mlp_glu","title":"<code>prune_model_mlp_glu</code>","text":"<pre><code>def prune_model_mlp_glu(\n    model: PreTrainedModel,\n    neuron_selection_method: str = \"MAW\",\n    pruning_percentage: Optional[float] = 10,\n    expansion_rate: Optional[float] = None,\n    show_progress: bool = True,\n) -&gt; PreTrainedModel:\n    \"\"\"\n    Prune the MLP layers in a model with GLU architecture.\n\n    Args:\n        model: Pre-trained model to prune\n        neuron_selection_method: Method to use for calculating neuron importance (\"MAW\", \"VOW\", or \"PON\")\n        pruning_percentage: Percentage of neurons to prune (0-100)\n        expansion_rate: Target expansion rate in percentage (mutually exclusive with pruning_percentage)\n        show_progress: Whether to show progress during pruning\n\n    Returns:\n        model: Pruned model\n    \"\"\"\n</code></pre>"},{"location":"api/#prune_neuron_pairs","title":"<code>prune_neuron_pairs</code>","text":"<pre><code>def prune_neuron_pairs(\n    mlp: nn.Module,\n    prune_percentage: float,\n    importance_fn: Callable = compute_neuron_pair_importance_maw\n) -&gt; Tuple[nn.Linear, nn.Linear, nn.Linear, int]:\n    \"\"\"\n    Prune a specific percentage of neurons from the MLP layers (GLU architecture).\n\n    Args:\n        mlp: MLP module containing gate_proj, up_proj, and down_proj layers\n        prune_percentage: Percentage of neurons to prune (0-100)\n        importance_fn: Function to compute neuron pair importance\n\n    Returns:\n        new_gate_proj: Pruned gate_proj layer\n        new_up_proj: Pruned up_proj layer\n        new_down_proj: Pruned down_proj layer\n        k: New intermediate size after pruning\n    \"\"\"\n</code></pre>"},{"location":"api/#calculate_pruning_percentage_from_expansion_rate","title":"<code>calculate_pruning_percentage_from_expansion_rate</code>","text":"<pre><code>def calculate_pruning_percentage_from_expansion_rate(\n    current_intermediate_size: int,\n    current_hidden_size: int,\n    target_expansion_rate: float\n) -&gt; float:\n    \"\"\"\n    Calculate the pruning percentage needed to achieve a target expansion rate.\n\n    Args:\n        current_intermediate_size: Current size of the intermediate layer\n        current_hidden_size: Current size of the hidden layer\n        target_expansion_rate: Target expansion rate in percentage (e.g., 140 for 140%)\n\n    Returns:\n        pruning_percentage: Percentage of neurons to prune\n    \"\"\"\n</code></pre>"},{"location":"api/#neuron-importance-functions","title":"Neuron Importance Functions","text":"<pre><code>def compute_neuron_pair_importance_maw(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Maximum Absolute Weight method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n\ndef compute_neuron_pair_importance_vow(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Variance of Weights method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n\ndef compute_neuron_pair_importance_pon(gate_weight: torch.Tensor, up_weight: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute neuron pair importance scores using Product of Norms method.\n\n    Args:\n        gate_weight: Weight matrix from the gate_proj layer\n        up_weight: Weight matrix from the up_proj layer\n\n    Returns:\n        importance_scores: Importance scores for each neuron pair\n    \"\"\"\n</code></pre>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#validate_model_for_glu_pruning","title":"<code>validate_model_for_glu_pruning</code>","text":"<pre><code>def validate_model_for_glu_pruning(model: PreTrainedModel) -&gt; bool:\n    \"\"\"\n    Validate that a model is compatible with GLU pruning.\n\n    Args:\n        model: Model to validate\n\n    Returns:\n        bool: True if the model is compatible, False otherwise\n    \"\"\"\n</code></pre>"},{"location":"api/#get_model_layers","title":"<code>get_model_layers</code>","text":"<pre><code>def get_model_layers(model: PreTrainedModel) -&gt; List[Any]:\n    \"\"\"\n    Extract transformer layers from a pre-trained model.\n    Currently supports LLaMA, Mistral, and similar model architectures.\n\n    Args:\n        model: Pre-trained model\n\n    Returns:\n        List of decoder layers that contain MLP blocks\n    \"\"\"\n</code></pre>"},{"location":"api/#count_parameters","title":"<code>count_parameters</code>","text":"<pre><code>def count_parameters(model: torch.nn.Module) -&gt; int:\n    \"\"\"\n    Count the number of trainable parameters in a model.\n\n    Args:\n        model: PyTorch model\n\n    Returns:\n        Number of trainable parameters\n    \"\"\"\n</code></pre>"},{"location":"api/#get_pruning_statistics","title":"<code>get_pruning_statistics</code>","text":"<pre><code>def get_pruning_statistics(\n    original_model: torch.nn.Module,\n    pruned_model: torch.nn.Module,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculate statistics about the pruning operation.\n\n    Args:\n        original_model: Original model before pruning\n        pruned_model: Model after pruning\n\n    Returns:\n        Dictionary containing pruning statistics\n    \"\"\"\n</code></pre>"},{"location":"api/#evaluation-module","title":"Evaluation Module","text":""},{"location":"api/#time_inference","title":"<code>time_inference</code>","text":"<pre><code>def time_inference(\n    model: PreTrainedModel,\n    tokenizer: AutoTokenizer,\n    prompt: str,\n    max_new_tokens: int = 100,\n    num_runs: int = 5,\n    warmup_runs: int = 2,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Measure inference time for a model.\n\n    Args:\n        model: Model to evaluate\n        tokenizer: Tokenizer to use\n        prompt: Input prompt for generation\n        max_new_tokens: Maximum number of tokens to generate\n        num_runs: Number of inference runs to average over\n        warmup_runs: Number of initial runs to discard (for warm-up)\n\n    Returns:\n        Dictionary containing timing results\n    \"\"\"\n</code></pre>"},{"location":"api/#compare_models_inference","title":"<code>compare_models_inference</code>","text":"<pre><code>def compare_models_inference(\n    original_model: PreTrainedModel,\n    pruned_model: PreTrainedModel,\n    tokenizer: AutoTokenizer,\n    prompts: List[str],\n    max_new_tokens: int = 100,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Compare inference performance between original and pruned models.\n\n    Args:\n        original_model: Original model before pruning\n        pruned_model: Model after pruning\n        tokenizer: Tokenizer to use\n        prompts: List of input prompts for generation\n        max_new_tokens: Maximum number of tokens to generate\n\n    Returns:\n        Dictionary containing comparison results\n    \"\"\"\n</code></pre>"},{"location":"api/#command-line-interface","title":"Command-Line Interface","text":"<p>The CLI provides several commands:</p>"},{"location":"api/#prune","title":"<code>prune</code>","text":"<pre><code>optipfair prune --model-path MODEL_PATH --output-path OUTPUT_PATH \n    [--pruning-type {MLP_GLU}] \n    [--method {MAW,VOW,PON}] \n    [--pruning-percentage PERCENTAGE] \n    [--expansion-rate RATE] \n    [--device DEVICE] \n    [--dtype {auto,float32,float16,bfloat16}] \n    [--verbose/--quiet]\n</code></pre>"},{"location":"api/#analyze","title":"<code>analyze</code>","text":"<pre><code>optipfair analyze --model-path MODEL_PATH \n    [--device DEVICE]\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to OptiPFair! We welcome contributions from everyone.</p> <p>For detailed guidelines on how to contribute, please see our CONTRIBUTING.md file in the repository.</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:    <pre><code>git clone https://github.com/peremartra/optipfair.git\ncd optipfair\n</code></pre></li> <li>Install development dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Create a new branch for your feature or bugfix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></li> <li>Make your changes, add tests, and ensure all tests pass:    <pre><code>pytest tests/\n</code></pre></li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<p>Our development workflow follows these steps:</p> <ol> <li>Open an Issue: Start by opening an issue describing the feature or bug</li> <li>Discussion: Discuss the approach with maintainers and community</li> <li>Implementation: Make your changes with tests and documentation</li> <li>Pull Request: Submit a PR referencing the original issue</li> <li>Review: Address any feedback from code review</li> <li>Merge: Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>When adding new features, please update the documentation as well. We use MkDocs for our documentation.</p> <p>To preview documentation changes locally:</p> <pre><code># Install MkDocs and required plugins\npip install mkdocs mkdocs-material mkdocstrings\n\n# Serve the documentation\nmkdocs serve\n</code></pre> <p>Then open your browser to http://127.0.0.1:8000/ to see the documentation site.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools to enforce code style:</p> <ul> <li><code>black</code> for code formatting</li> <li><code>isort</code> for import sorting</li> <li><code>flake8</code> for linting</li> <li><code>mypy</code> for type checking</li> </ul> <p>You can run these checks with:</p> <pre><code># Format code\nblack optipfair tests examples\nisort optipfair tests examples\n\n# Check code\nflake8 optipfair tests examples\nmypy optipfair\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>This page provides detailed examples of using OptiPFair for pruning language models.</p>"},{"location":"examples/#basic-pruning-example","title":"Basic Pruning Example","text":"<p>This example demonstrates how to prune a LLaMA model using the default settings:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Get original parameter count\noriginal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Original model parameters: {original_params:,}\")\n\n# Simple prompt for testing\nprompt = \"Paris is the capital of\"\n\n# Test original model\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(inputs.input_ids, max_length=50)\noriginal_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Original model output: {original_output}\")\n\n# Apply pruning with default settings (10% pruning, MAW method)\npruned_model = prune_model(model)\n\n# Get pruned parameter count\npruned_params = sum(p.numel() for p in pruned_model.parameters())\nprint(f\"Pruned model parameters: {pruned_params:,}\")\nprint(f\"Reduction: {original_params - pruned_params:,} ({(original_params - pruned_params) / original_params * 100:.2f}%)\")\n\n# Test pruned model\noutputs = pruned_model.generate(inputs.input_ids, max_length=50)\npruned_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Pruned model output: {pruned_output}\")\n\n# Save pruned model\npruned_model.save_pretrained(\"./pruned-llama\")\ntokenizer.save_pretrained(\"./pruned-llama\")\n</code></pre>"},{"location":"examples/#comparing-neuron-selection-methods","title":"Comparing Neuron Selection Methods","text":"<p>This example compares the results of different neuron selection methods:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nimport time\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Test prompt\nprompt = \"The capital of France is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Original model output\noriginal_output = tokenizer.decode(\n    original_model.generate(inputs.input_ids, max_length=50)[0], \n    skip_special_tokens=True\n)\nprint(f\"Original: {original_output}\")\n\n# Compare different neuron selection methods\nmethods = [\"MAW\", \"VOW\", \"PON\"]\npruning_percentage = 20\n\nresults = {}\n\nfor method in methods:\n    print(f\"\\nPruning with {method} method...\")\n\n    # Create a fresh copy of the model for this method\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    # Apply pruning\n    pruned_model, stats = prune_model(\n        model=model,\n        pruning_type=\"MLP_GLU\",\n        neuron_selection_method=method,\n        pruning_percentage=pruning_percentage,\n        show_progress=True,\n        return_stats=True\n    )\n\n    # Test generation\n    start_time = time.time()\n    outputs = pruned_model.generate(inputs.input_ids, max_length=50)\n    end_time = time.time()\n\n    pruned_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Store results\n    results[method] = {\n        \"output\": pruned_output,\n        \"parameters\": stats[\"pruned_parameters\"],\n        \"reduction\": stats[\"percentage_reduction\"],\n        \"inference_time\": end_time - start_time\n    }\n\n    print(f\"{method} output: {pruned_output}\")\n    print(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n    print(f\"Inference time: {end_time - start_time:.4f}s\")\n\n# Compare results\nprint(\"\\n===== COMPARISON =====\")\nfor method, data in results.items():\n    print(f\"\\n{method}:\")\n    print(f\"  Output: {data['output'][:100]}...\")\n    print(f\"  Parameter reduction: {data['reduction']:.2f}%\")\n    print(f\"  Inference time: {data['inference_time']:.4f}s\")\n</code></pre>"},{"location":"examples/#pruning-with-target-expansion-rate","title":"Pruning with Target Expansion Rate","text":"<p>This example demonstrates pruning a model to achieve a specific expansion rate:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nfrom optipfair.pruning.utils import get_model_layers\n\n# Load model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Analyze the model's current expansion rate\nlayers = get_model_layers(model)\nfirst_mlp = layers[0].mlp\nhidden_size = first_mlp.gate_proj.in_features\nintermediate_size = first_mlp.gate_proj.out_features\ncurrent_expansion_rate = (intermediate_size / hidden_size) * 100\n\nprint(f\"Model: {model_name}\")\nprint(f\"Hidden size: {hidden_size}\")\nprint(f\"Intermediate size: {intermediate_size}\")\nprint(f\"Current expansion rate: {current_expansion_rate:.2f}%\")\n\n# Set target expansion rate (e.g., 200% instead of the typical 400% for LLaMA)\ntarget_expansion_rate = 200.0\nprint(f\"Target expansion rate: {target_expansion_rate:.2f}%\")\n\n# Apply pruning with target expansion rate\npruned_model, stats = prune_model(\n    model=model,\n    pruning_type=\"MLP_GLU\",\n    neuron_selection_method=\"MAW\",\n    expansion_rate=target_expansion_rate,\n    show_progress=True,\n    return_stats=True\n)\n\n# Verify the new expansion rate\nlayers = get_model_layers(pruned_model)\nfirst_mlp = layers[0].mlp\nnew_intermediate_size = first_mlp.gate_proj.out_features\nnew_expansion_rate = (new_intermediate_size / hidden_size) * 100\n\nprint(f\"\\nAfter pruning:\")\nprint(f\"New intermediate size: {new_intermediate_size}\")\nprint(f\"New expansion rate: {new_expansion_rate:.2f}%\")\nprint(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n\n# Test generation\nprompt = \"The capital of France is\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\noriginal_output = tokenizer.decode(\n    original_model.generate(inputs.input_ids, max_length=50)[0],\n    skip_special_tokens=True\n)\n\npruned_output = tokenizer.decode(\n    pruned_model.generate(inputs.input_ids, max_length=50)[0],\n    skip_special_tokens=True\n)\n\nprint(f\"\\nOriginal: {original_output}\")\nprint(f\"Pruned: {pruned_output}\")\n\n# Save pruned model\npruned_model.save_pretrained(f\"./llama-er{int(target_expansion_rate)}\")\ntokenizer.save_pretrained(f\"./llama-er{int(target_expansion_rate)}\")\n</code></pre>"},{"location":"examples/#benchmarking-pruned-models","title":"Benchmarking Pruned Models","text":"<p>This example demonstrates how to benchmark and compare the performance of pruned models:</p> <pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair import prune_model\nfrom optipfair.evaluation.benchmarks import compare_models_inference\n\n# Load original model\nmodel_name = \"meta-llama/Llama-3.2-1B\"\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create pruned models with different settings\npruning_percentages = [10, 20, 30, 40]\npruned_models = {}\n\nfor percentage in pruning_percentages:\n    print(f\"Pruning model with {percentage}% pruning...\")\n\n    # Create a fresh copy of the model\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    # Apply pruning\n    pruned_model, stats = prune_model(\n        model=model,\n        pruning_type=\"MLP_GLU\",\n        neuron_selection_method=\"MAW\",\n        pruning_percentage=percentage,\n        show_progress=True,\n        return_stats=True\n    )\n\n    pruned_models[percentage] = {\n        \"model\": pruned_model,\n        \"stats\": stats\n    }\n\n    print(f\"Parameter reduction: {stats['percentage_reduction']:.2f}%\")\n\n# Test prompts for benchmarking\ntest_prompts = [\n    \"The capital of France is\",\n    \"Machine learning is a field of\",\n    \"The fastest land animal is the\",\n    \"In physics, the theory of relativity\",\n    \"The industrial revolution began in\"\n]\n\n# Benchmark each model\nprint(\"\\nBenchmarking models...\")\n\nresults = {}\nfor percentage, data in pruned_models.items():\n    print(f\"\\nEvaluating {percentage}% pruned model...\")\n\n    comparison = compare_models_inference(\n        original_model=original_model,\n        pruned_model=data[\"model\"],\n        tokenizer=tokenizer,\n        prompts=test_prompts,\n        max_new_tokens=100\n    )\n\n    results[percentage] = comparison\n\n    print(f\"Speedup: {comparison['speedup']:.2f}x\")\n    print(f\"Tokens per second improvement: {comparison['tps_improvement_percent']:.2f}%\")\n\n# Print summary\nprint(\"\\n===== PERFORMANCE SUMMARY =====\")\nprint(f\"{'Pruning %':&lt;10} {'Param Reduction':&lt;20} {'Speedup':&lt;10} {'TPS Improvement':&lt;20}\")\nprint(\"-\" * 60)\n\nfor percentage in pruning_percentages:\n    param_reduction = pruned_models[percentage][\"stats\"][\"percentage_reduction\"]\n    speedup = results[percentage][\"speedup\"]\n    tps_improvement = results[percentage][\"tps_improvement_percent\"]\n\n    print(f\"{percentage:&lt;10} {param_reduction:&lt;20.2f}% {speedup:&lt;10.2f}x {tps_improvement:&lt;20.2f}%\")\n</code></pre>"},{"location":"examples/#using-the-cli-for-multiple-models","title":"Using the CLI for Multiple Models","text":"<p>This bash script demonstrates how to use the OptiPFair CLI to prune multiple models:</p> <pre><code>#!/bin/bash\n\n# Models to prune\nMODELS=(\n    \"meta-llama/Llama-3.2-1B\"\n    \"meta-llama/Llama-3.2-3B\"\n)\n\n# Pruning percentages to try\nPERCENTAGES=(10 20 30)\n\n# Method to use\nMETHOD=\"MAW\"\n\n# Output directory\nOUTPUT_DIR=\"./pruned-models\"\nmkdir -p \"$OUTPUT_DIR\"\n\n# Log file\nLOG_FILE=\"$OUTPUT_DIR/pruning_log.txt\"\necho \"OptiPFair Pruning Log - $(date)\" &gt; \"$LOG_FILE\"\n\n# Loop through models and percentages\nfor MODEL in \"${MODELS[@]}\"; do\n    MODEL_NAME=$(basename \"$MODEL\")\n\n    for PERCENTAGE in \"${PERCENTAGES[@]}\"; do\n        OUTPUT_PATH=\"$OUTPUT_DIR/${MODEL_NAME}_pruned_${PERCENTAGE}p\"\n\n        echo \"Pruning $MODEL with $PERCENTAGE% using $METHOD method...\"\n        echo \"Output will be saved to $OUTPUT_PATH\"\n\n        # Log the command\n        echo -e \"\\n\\n===== $MODEL - $PERCENTAGE% - $(date) =====\" &gt;&gt; \"$LOG_FILE\"\n\n        # Run the pruning command\n        optipfair prune \\\n            --model-path \"$MODEL\" \\\n            --pruning-type MLP_GLU \\\n            --method \"$METHOD\" \\\n            --pruning-percentage \"$PERCENTAGE\" \\\n            --output-path \"$OUTPUT_PATH\" \\\n            --device cuda | tee -a \"$LOG_FILE\"\n\n        echo \"Completed pruning $MODEL with $PERCENTAGE%\"\n        echo \"-------------------------------------------\"\n    done\ndone\n\necho \"All pruning jobs completed. Results saved to $OUTPUT_DIR\"\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<p>OptiPFair requires:</p> <ul> <li>Python 3.8 or higher</li> <li>PyTorch 1.10.0 or higher</li> <li>Transformers 4.25.0 or higher</li> </ul>"},{"location":"installation/#installing-from-pypi","title":"Installing from PyPI","text":"<pre><code>pip install optipfair\n</code></pre>"},{"location":"installation/#installing-from-source","title":"Installing from Source","text":"<p>You can install the latest development version from GitHub:</p> <pre><code>git clone https://github.com/yourusername/optipfair.git\ncd optipfair\npip install -e .\n</code></pre> <p>OptiPFair will automatically use available GPUs when loading and processing models.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This document outlines the planned features and improvements for OptiPFair.</p>"},{"location":"roadmap/#mid-term-goals-0-6-months","title":"Mid-term Goals (0-6 months)","text":""},{"location":"roadmap/#version-020","title":"Version 0.2.0","text":"<ul> <li>Attention Mechanism Pruning: Implement pruning techniques for attention layers</li> <li>Transformer Block Pruning: Implement pruning techniques for entire transformer blocks</li> <li>Bias Visualization: Implement visualizations for Bias in pair of prompts</li> </ul>"},{"location":"roadmap/#version-030","title":"Version 0.3.0","text":"<ul> <li>Comprehensive Benchmarks: Add integration with common LLM benchmarks</li> <li>NO GLU Models: Implement pruning techniques for older models (no GLU)</li> <li>Improved Documentation: Add more examples and tutorials</li> </ul>"},{"location":"roadmap/#version-040","title":"Version 0.4.0","text":"<ul> <li>Configuration Presets: Provide optimized pruning configurations for different model families</li> <li>Visualization Tools: Add tools for visualizing neuron importance and pruning impact</li> </ul>"},{"location":"roadmap/#version-050","title":"Version 0.5.0","text":"<ul> <li>Fairness prunning: consider bias in pruning. </li> </ul>"},{"location":"roadmap/#long-term-goals-6-months","title":"Long-term Goals (6+ months)","text":""},{"location":"roadmap/#version-100","title":"Version 1.0.0","text":"<ul> <li>Distributed Pruning: Support for pruning very large models across multiple GPUs</li> <li>Dynamic Pruning: Techniques for runtime pruning based on inference context</li> <li>Knowledge Distillation: Integration with knowledge distillation techniques</li> <li>Non-transformer Models: Extend support to other model architectures</li> <li>Automated Pruning: Implement algorithms to automatically determine optimal pruning parameters</li> <li>Iterative Pruning: Support for gradual pruning over multiple iterations</li> <li>Fine-tuning Integration: Direct integration with fine-tuning workflows</li> </ul>"},{"location":"roadmap/#community-suggestions","title":"Community Suggestions","text":"<p>We welcome community input on our roadmap! If you have suggestions for features or improvements, please submit them as issues on our GitHub repository with the label \"enhancement\".</p>"},{"location":"usage/","title":"Usage Guide","text":""},{"location":"usage/#python-api","title":"Python API","text":"<p>OptiPFair provides a simple Python API for pruning models.</p>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<pre><code>from transformers import AutoModelForCausalLM\nfrom optipfair import prune_model\n\n# Load a pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\n# Prune the model with default settings (10% pruning, MAW method)\npruned_model = prune_model(model=model)\n\n# Save the pruned model\npruned_model.save_pretrained(\"./pruned-model\")\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":"<pre><code># Prune with custom settings\npruned_model, stats = prune_model(\n    model=model,\n    pruning_type=\"MLP_GLU\",              # Type of pruning to apply\n    neuron_selection_method=\"MAW\",       # Method to calculate neuron importance\n    pruning_percentage=20,               # Percentage of neurons to prune\n    # expansion_rate=140,                # Alternatively, specify target expansion rate\n    show_progress=True,                  # Show progress during pruning\n    return_stats=True                    # Return pruning statistics\n)\n\n# Print pruning statistics\nprint(f\"Original parameters: {stats['original_parameters']:,}\")\nprint(f\"Pruned parameters: {stats['pruned_parameters']:,}\")\nprint(f\"Reduction: {stats['reduction']:,} parameters ({stats['percentage_reduction']:.2f}%)\")\n</code></pre>"},{"location":"usage/#command-line-interface","title":"Command-Line Interface","text":"<p>OptiPFair provides a command-line interface for pruning models:</p>"},{"location":"usage/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Prune a model with default settings (10% pruning, MAW method)\noptipfair prune --model-path meta-llama/Llama-3.2-1B --output-path ./pruned-model\n</code></pre>"},{"location":"usage/#advanced-usage_1","title":"Advanced Usage","text":"<pre><code># Prune with custom settings\noptipfair prune \\\n  --model-path meta-llama/Llama-3.2-1B \\\n  --pruning-type MLP_GLU \\\n  --method MAW \\\n  --pruning-percentage 20 \\\n  --output-path ./pruned-model \\\n  --device cuda \\\n  --dtype float16\n</code></pre>"},{"location":"usage/#analyzing-a-model","title":"Analyzing a Model","text":"<pre><code># Analyze a model's architecture and parameter distribution\noptipfair analyze --model-path meta-llama/Llama-3.2-1B\n</code></pre>"},{"location":"usage/#neuron-selection-methods","title":"Neuron Selection Methods","text":"<p>OptiPFair supports three methods for calculating neuron importance:</p>"},{"location":"usage/#maw-maximum-absolute-weight","title":"MAW (Maximum Absolute Weight)","text":"<p>The MAW method identifies neurons based on the maximum absolute weight values in their connections. This is typically the most effective method for GLU architectures.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"MAW\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#vow-variance-of-weights","title":"VOW (Variance of Weights)","text":"<p>The VOW method identifies neurons based on the variance of their weight values. This can be useful for certain specific architectures.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"VOW\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#pon-product-of-norms","title":"PON (Product of Norms)","text":"<p>The PON method uses the product of L1 norms to identify important neurons. This is an alternative approach that may be useful in certain contexts.</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    neuron_selection_method=\"PON\",\n    pruning_percentage=20\n)\n</code></pre>"},{"location":"usage/#pruning-percentage-vs-expansion-rate","title":"Pruning Percentage vs Expansion Rate","text":"<p>OptiPFair supports two ways to specify the pruning target:</p>"},{"location":"usage/#pruning-percentage","title":"Pruning Percentage","text":"<p>Directly specify what percentage of neurons to remove:</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    pruning_percentage=20  # Remove 20% of neurons\n)\n</code></pre>"},{"location":"usage/#expansion-rate","title":"Expansion Rate","text":"<p>Specify the target expansion rate (ratio of intermediate size to hidden size) as a percentage:</p> <pre><code>pruned_model = prune_model(\n    model=model,\n    expansion_rate=140  # Target 140% expansion rate\n)\n</code></pre> <p>This approach is often more intuitive when comparing across different model scales.</p>"},{"location":"usage/#evaluating-pruned-models","title":"Evaluating Pruned Models","text":"<p>After pruning, you can use OptiPFair's evaluation tools to assess the performance of the pruned model:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom optipfair.evaluation.benchmarks import time_inference, compare_models_inference\n\n# Load original and pruned models\noriginal_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\npruned_model = AutoModelForCausalLM.from_pretrained(\"./pruned-model\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n\n# Compare inference speed\ncomparison = compare_models_inference(\n    original_model,\n    pruned_model,\n    tokenizer,\n    prompts=[\"Paris is the capital of\", \"The speed of light is approximately\"],\n    max_new_tokens=50\n)\n\nprint(f\"Speedup: {comparison['speedup']:.2f}x\")\nprint(f\"Tokens per second improvement: {comparison['tps_improvement_percent']:.2f}%\")\n</code></pre>"}]}